{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla M40 (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "\n",
    "##Keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout, Flatten, Activation\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.utils.visualize_util as vutil\n",
    "from keras.optimizers import Adadelta, Adam, SGD, RMSprop, Adadelta\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils.visualize_util import plot,model_to_dot\n",
    "from keras.layers.convolutional import Convolution2D,MaxPooling2D,Convolution1D,MaxPooling1D\n",
    "from IPython.display import SVG\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genePD = pd.read_table(\"P50Gene.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70534, 75)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#genePD.iloc[:30,:]\n",
    "genePD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#genePD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "(68,)\n"
     ]
    }
   ],
   "source": [
    "header = genePD.columns.values[7:]\n",
    "geneLabel = np.zeros_like(header).astype(int)\n",
    "print(type(header[0]))\n",
    "print geneLabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(header.shape[0]):\n",
    "    if(\"P00\" in header[i]):\n",
    "        geneLabel[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#con = []\n",
    "#for i in range(header.shape[0]):\n",
    "#    con.append([header[i],geneLabel[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#con[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#con[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#genePD.iloc[:,7:]\n",
    "genePD1 = genePD.iloc[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 68)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(geneLabel).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10334_P0002-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10335_P0004-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10336_P0006-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10337_P0008-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10338_P0010-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10339_P0011-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10340_P0015-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10341_P0017-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10342_P0019-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10343_P0020-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10344_P0023-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10345_P0024-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10346_P0025-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10347_P0027-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10348_P0029-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10349_P0030-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10350_P0031-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10351_P0032-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10352_P0042-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th>10353_P0044-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 4.87136</td>\n",
       "      <td> 4.78377</td>\n",
       "      <td> 4.53131</td>\n",
       "      <td> 4.61647</td>\n",
       "      <td> 4.59979</td>\n",
       "      <td> 4.69951</td>\n",
       "      <td> 4.44912</td>\n",
       "      <td> 4.73384</td>\n",
       "      <td> 4.69539</td>\n",
       "      <td> 4.85434</td>\n",
       "      <td> 4.76349</td>\n",
       "      <td> 4.44754</td>\n",
       "      <td> 4.78389</td>\n",
       "      <td> 4.86275</td>\n",
       "      <td> 4.96451</td>\n",
       "      <td> 4.66143</td>\n",
       "      <td> 4.76177</td>\n",
       "      <td> 4.63940</td>\n",
       "      <td> 4.57855</td>\n",
       "      <td> 5.16820</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2.83152</td>\n",
       "      <td> 2.54463</td>\n",
       "      <td> 2.15487</td>\n",
       "      <td> 2.21499</td>\n",
       "      <td> 2.30246</td>\n",
       "      <td> 2.19925</td>\n",
       "      <td> 2.26428</td>\n",
       "      <td> 2.31739</td>\n",
       "      <td> 2.18415</td>\n",
       "      <td> 3.16996</td>\n",
       "      <td> 2.80937</td>\n",
       "      <td> 2.23663</td>\n",
       "      <td> 2.88632</td>\n",
       "      <td> 2.53508</td>\n",
       "      <td> 2.84542</td>\n",
       "      <td> 2.62939</td>\n",
       "      <td> 2.77089</td>\n",
       "      <td> 2.49775</td>\n",
       "      <td> 2.49123</td>\n",
       "      <td> 2.96151</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 7.44357</td>\n",
       "      <td> 7.57288</td>\n",
       "      <td> 8.15854</td>\n",
       "      <td> 9.23785</td>\n",
       "      <td> 7.76936</td>\n",
       "      <td> 8.39819</td>\n",
       "      <td> 8.81647</td>\n",
       "      <td> 8.00555</td>\n",
       "      <td> 8.10353</td>\n",
       "      <td> 5.09584</td>\n",
       "      <td> 7.24615</td>\n",
       "      <td> 8.82426</td>\n",
       "      <td> 6.85516</td>\n",
       "      <td> 7.08659</td>\n",
       "      <td> 6.31959</td>\n",
       "      <td> 7.82094</td>\n",
       "      <td> 7.65633</td>\n",
       "      <td> 8.75288</td>\n",
       "      <td> 8.88902</td>\n",
       "      <td> 5.50549</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 4.79338</td>\n",
       "      <td> 4.52983</td>\n",
       "      <td> 4.33860</td>\n",
       "      <td> 4.08913</td>\n",
       "      <td> 4.20969</td>\n",
       "      <td> 4.26832</td>\n",
       "      <td> 4.03489</td>\n",
       "      <td> 4.51406</td>\n",
       "      <td> 4.29894</td>\n",
       "      <td> 4.74181</td>\n",
       "      <td> 4.48198</td>\n",
       "      <td> 4.00659</td>\n",
       "      <td> 4.78036</td>\n",
       "      <td> 4.34761</td>\n",
       "      <td> 4.46845</td>\n",
       "      <td> 4.34083</td>\n",
       "      <td> 4.42759</td>\n",
       "      <td> 4.39995</td>\n",
       "      <td> 4.17632</td>\n",
       "      <td> 4.70299</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 5.25525</td>\n",
       "      <td> 5.12963</td>\n",
       "      <td> 5.09532</td>\n",
       "      <td> 5.16179</td>\n",
       "      <td> 4.97679</td>\n",
       "      <td> 5.10457</td>\n",
       "      <td> 4.93976</td>\n",
       "      <td> 5.20587</td>\n",
       "      <td> 5.34848</td>\n",
       "      <td> 5.33516</td>\n",
       "      <td> 5.30964</td>\n",
       "      <td> 4.96604</td>\n",
       "      <td> 5.24186</td>\n",
       "      <td> 4.98327</td>\n",
       "      <td> 5.30314</td>\n",
       "      <td> 5.08783</td>\n",
       "      <td> 5.01474</td>\n",
       "      <td> 5.31235</td>\n",
       "      <td> 5.09411</td>\n",
       "      <td> 5.22861</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10334_P0002-T1_(HTA-2_0).CEL.pimg  10335_P0004-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.87136                            4.78377   \n",
       "1                            2.83152                            2.54463   \n",
       "2                            7.44357                            7.57288   \n",
       "3                            4.79338                            4.52983   \n",
       "4                            5.25525                            5.12963   \n",
       "\n",
       "   10336_P0006-T1_(HTA-2_0).CEL.pimg  10337_P0008-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.53131                            4.61647   \n",
       "1                            2.15487                            2.21499   \n",
       "2                            8.15854                            9.23785   \n",
       "3                            4.33860                            4.08913   \n",
       "4                            5.09532                            5.16179   \n",
       "\n",
       "   10338_P0010-T1_(HTA-2_0).CEL.pimg  10339_P0011-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.59979                            4.69951   \n",
       "1                            2.30246                            2.19925   \n",
       "2                            7.76936                            8.39819   \n",
       "3                            4.20969                            4.26832   \n",
       "4                            4.97679                            5.10457   \n",
       "\n",
       "   10340_P0015-T1_(HTA-2_0).CEL.pimg  10341_P0017-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.44912                            4.73384   \n",
       "1                            2.26428                            2.31739   \n",
       "2                            8.81647                            8.00555   \n",
       "3                            4.03489                            4.51406   \n",
       "4                            4.93976                            5.20587   \n",
       "\n",
       "   10342_P0019-T1_(HTA-2_0).CEL.pimg  10343_P0020-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.69539                            4.85434   \n",
       "1                            2.18415                            3.16996   \n",
       "2                            8.10353                            5.09584   \n",
       "3                            4.29894                            4.74181   \n",
       "4                            5.34848                            5.33516   \n",
       "\n",
       "   10344_P0023-T1_(HTA-2_0).CEL.pimg  10345_P0024-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.76349                            4.44754   \n",
       "1                            2.80937                            2.23663   \n",
       "2                            7.24615                            8.82426   \n",
       "3                            4.48198                            4.00659   \n",
       "4                            5.30964                            4.96604   \n",
       "\n",
       "   10346_P0025-T1_(HTA-2_0).CEL.pimg  10347_P0027-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.78389                            4.86275   \n",
       "1                            2.88632                            2.53508   \n",
       "2                            6.85516                            7.08659   \n",
       "3                            4.78036                            4.34761   \n",
       "4                            5.24186                            4.98327   \n",
       "\n",
       "   10348_P0029-T1_(HTA-2_0).CEL.pimg  10349_P0030-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.96451                            4.66143   \n",
       "1                            2.84542                            2.62939   \n",
       "2                            6.31959                            7.82094   \n",
       "3                            4.46845                            4.34083   \n",
       "4                            5.30314                            5.08783   \n",
       "\n",
       "   10350_P0031-T1_(HTA-2_0).CEL.pimg  10351_P0032-T1_(HTA-2_0).CEL.pimg  \\\n",
       "0                            4.76177                            4.63940   \n",
       "1                            2.77089                            2.49775   \n",
       "2                            7.65633                            8.75288   \n",
       "3                            4.42759                            4.39995   \n",
       "4                            5.01474                            5.31235   \n",
       "\n",
       "   10352_P0042-T1_(HTA-2_0).CEL.pimg  10353_P0044-T1_(HTA-2_0).CEL.pimg      \n",
       "0                            4.57855                            5.16820 ...  \n",
       "1                            2.49123                            2.96151 ...  \n",
       "2                            8.88902                            5.50549 ...  \n",
       "3                            4.17632                            4.70299 ...  \n",
       "4                            5.09411                            5.22861 ...  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genePD1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 70534)\n"
     ]
    }
   ],
   "source": [
    "genePD2 = genePD1.T\n",
    "print(genePD2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10334_P0002-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <td> 4.87136</td>\n",
       "      <td> 2.83152</td>\n",
       "      <td> 7.44357</td>\n",
       "      <td> 4.79338</td>\n",
       "      <td> 5.25525</td>\n",
       "      <td> 4.86576</td>\n",
       "      <td> 2.83494</td>\n",
       "      <td> 4.91114</td>\n",
       "      <td> 3.90338</td>\n",
       "      <td> 3.86068</td>\n",
       "      <td> 5.52244</td>\n",
       "      <td> 3.41357</td>\n",
       "      <td> 1.08527</td>\n",
       "      <td> 6.28080</td>\n",
       "      <td> 2.15164</td>\n",
       "      <td> 2.08403</td>\n",
       "      <td> 4.16124</td>\n",
       "      <td> 4.98600</td>\n",
       "      <td> 2.52245</td>\n",
       "      <td> 5.08710</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335_P0004-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <td> 4.78377</td>\n",
       "      <td> 2.54463</td>\n",
       "      <td> 7.57288</td>\n",
       "      <td> 4.52983</td>\n",
       "      <td> 5.12963</td>\n",
       "      <td> 4.74778</td>\n",
       "      <td> 2.54434</td>\n",
       "      <td> 4.58574</td>\n",
       "      <td> 3.99433</td>\n",
       "      <td> 3.54164</td>\n",
       "      <td> 5.14009</td>\n",
       "      <td> 3.27056</td>\n",
       "      <td> 1.32740</td>\n",
       "      <td> 5.86095</td>\n",
       "      <td> 1.96600</td>\n",
       "      <td> 1.85380</td>\n",
       "      <td> 3.59303</td>\n",
       "      <td> 5.17989</td>\n",
       "      <td> 2.79534</td>\n",
       "      <td> 5.25150</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336_P0006-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <td> 4.53131</td>\n",
       "      <td> 2.15487</td>\n",
       "      <td> 8.15854</td>\n",
       "      <td> 4.33860</td>\n",
       "      <td> 5.09532</td>\n",
       "      <td> 4.02288</td>\n",
       "      <td> 2.22244</td>\n",
       "      <td> 3.90208</td>\n",
       "      <td> 3.71936</td>\n",
       "      <td> 3.21449</td>\n",
       "      <td> 5.10882</td>\n",
       "      <td> 2.74734</td>\n",
       "      <td> 1.27408</td>\n",
       "      <td> 6.04253</td>\n",
       "      <td> 1.93392</td>\n",
       "      <td> 1.91717</td>\n",
       "      <td> 3.56547</td>\n",
       "      <td> 5.91583</td>\n",
       "      <td> 1.50201</td>\n",
       "      <td> 4.97899</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10337_P0008-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <td> 4.61647</td>\n",
       "      <td> 2.21499</td>\n",
       "      <td> 9.23785</td>\n",
       "      <td> 4.08913</td>\n",
       "      <td> 5.16179</td>\n",
       "      <td> 4.29374</td>\n",
       "      <td> 2.57431</td>\n",
       "      <td> 4.13444</td>\n",
       "      <td> 3.61667</td>\n",
       "      <td> 3.30135</td>\n",
       "      <td> 4.97830</td>\n",
       "      <td> 2.92673</td>\n",
       "      <td> 1.11526</td>\n",
       "      <td> 6.22503</td>\n",
       "      <td> 1.75350</td>\n",
       "      <td> 1.91137</td>\n",
       "      <td> 3.58365</td>\n",
       "      <td> 5.83059</td>\n",
       "      <td> 1.83321</td>\n",
       "      <td> 4.65183</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338_P0010-T1_(HTA-2_0).CEL.pimg</th>\n",
       "      <td> 4.59979</td>\n",
       "      <td> 2.30246</td>\n",
       "      <td> 7.76936</td>\n",
       "      <td> 4.20969</td>\n",
       "      <td> 4.97679</td>\n",
       "      <td> 4.52618</td>\n",
       "      <td> 1.94363</td>\n",
       "      <td> 4.36768</td>\n",
       "      <td> 3.50123</td>\n",
       "      <td> 3.05553</td>\n",
       "      <td> 5.26039</td>\n",
       "      <td> 3.14082</td>\n",
       "      <td> 1.54465</td>\n",
       "      <td> 5.76144</td>\n",
       "      <td> 1.71742</td>\n",
       "      <td> 1.90399</td>\n",
       "      <td> 3.99412</td>\n",
       "      <td> 5.64458</td>\n",
       "      <td> 1.96906</td>\n",
       "      <td> 4.90701</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0        1        2        3   \\\n",
       "10334_P0002-T1_(HTA-2_0).CEL.pimg  4.87136  2.83152  7.44357  4.79338   \n",
       "10335_P0004-T1_(HTA-2_0).CEL.pimg  4.78377  2.54463  7.57288  4.52983   \n",
       "10336_P0006-T1_(HTA-2_0).CEL.pimg  4.53131  2.15487  8.15854  4.33860   \n",
       "10337_P0008-T1_(HTA-2_0).CEL.pimg  4.61647  2.21499  9.23785  4.08913   \n",
       "10338_P0010-T1_(HTA-2_0).CEL.pimg  4.59979  2.30246  7.76936  4.20969   \n",
       "\n",
       "                                        4        5        6        7   \\\n",
       "10334_P0002-T1_(HTA-2_0).CEL.pimg  5.25525  4.86576  2.83494  4.91114   \n",
       "10335_P0004-T1_(HTA-2_0).CEL.pimg  5.12963  4.74778  2.54434  4.58574   \n",
       "10336_P0006-T1_(HTA-2_0).CEL.pimg  5.09532  4.02288  2.22244  3.90208   \n",
       "10337_P0008-T1_(HTA-2_0).CEL.pimg  5.16179  4.29374  2.57431  4.13444   \n",
       "10338_P0010-T1_(HTA-2_0).CEL.pimg  4.97679  4.52618  1.94363  4.36768   \n",
       "\n",
       "                                        8        9        10       11  \\\n",
       "10334_P0002-T1_(HTA-2_0).CEL.pimg  3.90338  3.86068  5.52244  3.41357   \n",
       "10335_P0004-T1_(HTA-2_0).CEL.pimg  3.99433  3.54164  5.14009  3.27056   \n",
       "10336_P0006-T1_(HTA-2_0).CEL.pimg  3.71936  3.21449  5.10882  2.74734   \n",
       "10337_P0008-T1_(HTA-2_0).CEL.pimg  3.61667  3.30135  4.97830  2.92673   \n",
       "10338_P0010-T1_(HTA-2_0).CEL.pimg  3.50123  3.05553  5.26039  3.14082   \n",
       "\n",
       "                                        12       13       14       15  \\\n",
       "10334_P0002-T1_(HTA-2_0).CEL.pimg  1.08527  6.28080  2.15164  2.08403   \n",
       "10335_P0004-T1_(HTA-2_0).CEL.pimg  1.32740  5.86095  1.96600  1.85380   \n",
       "10336_P0006-T1_(HTA-2_0).CEL.pimg  1.27408  6.04253  1.93392  1.91717   \n",
       "10337_P0008-T1_(HTA-2_0).CEL.pimg  1.11526  6.22503  1.75350  1.91137   \n",
       "10338_P0010-T1_(HTA-2_0).CEL.pimg  1.54465  5.76144  1.71742  1.90399   \n",
       "\n",
       "                                        16       17       18       19      \n",
       "10334_P0002-T1_(HTA-2_0).CEL.pimg  4.16124  4.98600  2.52245  5.08710 ...  \n",
       "10335_P0004-T1_(HTA-2_0).CEL.pimg  3.59303  5.17989  2.79534  5.25150 ...  \n",
       "10336_P0006-T1_(HTA-2_0).CEL.pimg  3.56547  5.91583  1.50201  4.97899 ...  \n",
       "10337_P0008-T1_(HTA-2_0).CEL.pimg  3.58365  5.83059  1.83321  4.65183 ...  \n",
       "10338_P0010-T1_(HTA-2_0).CEL.pimg  3.99412  5.64458  1.96906  4.90701 ...  \n",
       "\n",
       "[5 rows x 70534 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genePD2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting data into train and testing. 70% Training and 30% Testing..\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(genePD2, geneLabel, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((47, 70534), (21, 70534))\n",
      "((47,), (21,))\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((68, 70534), (68,))\n"
     ]
    }
   ],
   "source": [
    "X = genePD2.values\n",
    "Y = geneLabel\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "test_size = 0.30\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression-sklearn\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "loocv = cross_validation.LeaveOneOut(n=num_instances)\n",
    "kfold = cross_validation.ShuffleSplit(n=num_instances, n_iter=num_samples, test_size=test_size, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "#scoring = 'roc_auc'\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "#results = cross_validation.cross_val_score(model, X, Y, cv=loocv)\n",
    "#results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f\") % (results.mean())\n",
    "#print(\"AUC: %.3f\") % (results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904761904762\n",
      "[[ 5  2]\n",
      " [ 0 14]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83         7\n",
      "          1       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.92      0.90      0.90        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print metrics.accuracy_score(Y_test, predicted)\n",
    "print(matrix)\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761904761905\n",
      "[[ 4  3]\n",
      " [ 2 12]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.57      0.62         7\n",
      "          1       0.80      0.86      0.83        14\n",
      "\n",
      "avg / total       0.76      0.76      0.76        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print metrics.accuracy_score(Y_test, predicted)\n",
    "print(matrix)\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression-sklearn\n",
    "#logReg = LogisticRegression()\n",
    "#logReg.fit(X_train.values,Y_train)\n",
    "#Y_logPred = logReg.predict(X_test)\n",
    "#print metrics.accuracy_score(Y_test.reshape(len(Y_test),1),Y_logPred)\n",
    "#Confusion matrix of logistic regression\n",
    "#conf = metrics.confusion_matrix(Y_test,Y_logPred)\n",
    "#print conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Forest Classification\n",
    "#randForest = RandomForestClassifier(n_estimators=100)\n",
    "#randForest.fit(X_train,Y_train)\n",
    "#Y_randForestPred = randForest.predict(X_test)\n",
    "#print metrics.accuracy_score(Y_test,Y_randForestPred)\n",
    "#Confusion matrix for randomforest\n",
    "#conf2 = metrics.confusion_matrix(Y_test,Y_randForestPred)\n",
    "#print conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(genePD2, geneLabel, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 82.92% (16.92%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "#encoded_Y\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2), input_shape=(70534,))\n",
    "    #model.add(Dense(300, init= 'normal', activation= 'relu' ))\n",
    "    model.add(Dense(300, input_dim=70534, init= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, init= 'normal', activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, init= 'normal', activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init= 'normal' , activation= 'sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize' , StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(y=encoded_Y, n_folds=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_41 (Dense)                 (None, 300)           21160500    dense_input_11[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 300)           0           dense_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 100)           30100       dropout_31[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 100)           0           dense_42[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 20)            2020        dropout_32[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 20)            0           dense_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 1)             21          dropout_33[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 21192641\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"564pt\" viewBox=\"0.00 0.00 182.00 564.00\" width=\"182pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 560)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-560 178,-560 178,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140569776787216 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140569776787216</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-519 -0.5,-555 174.5,-555 174.5,-519 -0.5,-519\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-533.3\">dense_input_12 (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 140569776787344 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140569776787344</title>\n",
       "<polygon fill=\"none\" points=\"30,-445 30,-481 144,-481 144,-445 30,-445\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-459.3\">dense_45 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140569776787216&#45;&gt;140569776787344 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140569776787216-&gt;140569776787344</title>\n",
       "<path d=\"M87,-518.937C87,-510.807 87,-500.876 87,-491.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-491.441 87,-481.441 83.5001,-491.441 90.5001,-491.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140569858305296 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140569858305296</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-371 18.5,-407 155.5,-407 155.5,-371 18.5,-371\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-385.3\">dropout_34 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 140569776787344&#45;&gt;140569858305296 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140569776787344-&gt;140569858305296</title>\n",
       "<path d=\"M87,-444.937C87,-436.807 87,-426.876 87,-417.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-417.441 87,-407.441 83.5001,-417.441 90.5001,-417.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140570198317456 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140570198317456</title>\n",
       "<polygon fill=\"none\" points=\"30,-297 30,-333 144,-333 144,-297 30,-297\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-311.3\">dense_46 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140569858305296&#45;&gt;140570198317456 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140569858305296-&gt;140570198317456</title>\n",
       "<path d=\"M87,-370.937C87,-362.807 87,-352.876 87,-343.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-343.441 87,-333.441 83.5001,-343.441 90.5001,-343.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140570187480336 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140570187480336</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-223 18.5,-259 155.5,-259 155.5,-223 18.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-237.3\">dropout_35 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 140570198317456&#45;&gt;140570187480336 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140570198317456-&gt;140570187480336</title>\n",
       "<path d=\"M87,-296.937C87,-288.807 87,-278.876 87,-269.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-269.441 87,-259.441 83.5001,-269.441 90.5001,-269.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140570042177040 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140570042177040</title>\n",
       "<polygon fill=\"none\" points=\"30,-149 30,-185 144,-185 144,-149 30,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-163.3\">dense_47 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140570187480336&#45;&gt;140570042177040 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140570187480336-&gt;140570042177040</title>\n",
       "<path d=\"M87,-222.937C87,-214.807 87,-204.876 87,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-195.441 87,-185.441 83.5001,-195.441 90.5001,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140569774991504 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140569774991504</title>\n",
       "<polygon fill=\"none\" points=\"18.5,-75 18.5,-111 155.5,-111 155.5,-75 18.5,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-89.3\">dropout_36 (Dropout)</text>\n",
       "</g>\n",
       "<!-- 140570042177040&#45;&gt;140569774991504 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140570042177040-&gt;140569774991504</title>\n",
       "<path d=\"M87,-148.937C87,-140.807 87,-130.876 87,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-121.441 87,-111.441 83.5001,-121.441 90.5001,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140569858500560 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140569858500560</title>\n",
       "<polygon fill=\"none\" points=\"30,-1 30,-37 144,-37 144,-1 30,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"87\" y=\"-15.3\">dense_48 (Dense)</text>\n",
       "</g>\n",
       "<!-- 140569774991504&#45;&gt;140569858500560 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140569774991504-&gt;140569858500560</title>\n",
       "<path d=\"M87,-74.937C87,-66.8072 87,-56.8761 87,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"90.5001,-47.4406 87,-37.4407 83.5001,-47.4407 90.5001,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create_model().get_config()\n",
    "#SVG(vutil.to_graph(create_model(), recursive=True, show_shape=True).create(prog='dot', format=\"svg\"))\n",
    "#graph = to_graph(create_model(), show_shape=True)\n",
    "#plot(create_model(),to_file='model.png')\n",
    "SVG(model_to_dot(create_model()).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((68, 70534), (68,))\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def continous_decay(epoch): \n",
    "    return learning_rate / float(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd90a21c790>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUucZ/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f39c8dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decay = []\n",
    "learning_rate = 0.01\n",
    "for i in range(1,20):\n",
    "    decay.append(continous_decay(i))\n",
    "plt.plot(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 86.49% (8.90%)\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs #Continous \n",
    "#decay_rate = continous_decay\n",
    "momentum = 0.8\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "#encoded_Y\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "#adadelta = Adadelta(lr=learning_rate, decay=decay_rate)\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2), input_shape=(70534,))\n",
    "    #model.add(Dense(300, init= 'normal', activation= 'relu' ))\n",
    "    model.add(Dense(300, input_dim=70534, init= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, init= 'normal', activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, init= 'normal', activation= 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init= 'normal' , activation= 'sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'binary_crossentropy' , optimizer= 'sgd' , metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize' , StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, nb_epoch=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(y=encoded_Y, n_folds=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "#print model.predict_classes(X, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate schedule, drop learning rate discrete instead of continous i.e. epoch 1-5,lr=0.1; epoch 5-10,lr=0.01\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd8f8796ad0>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEACAYAAABS29YJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJKk//w+ErH7tt/IS3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd90c0b9ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decay = []\n",
    "for i in range(20):\n",
    "    decay.append(step_decay(i))\n",
    "plt.plot(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47 samples, validate on 21 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 14.1147 - acc: 0.1064 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "0s - loss: 15.4322 - acc: 0.0426 - val_loss: 1.0000e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "#decay_rate = continous_decay\n",
    "momentum = 0.8\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "#encoded_Y\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#encoded_Y_one_hot = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "#adadelta = Adadelta(lr=learning_rate, decay=decay_rate)\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2), input_shape=(70534,))\n",
    "    #model.add(Dense(300, init= 'normal', activation= 'relu' ))\n",
    "    model.add(Dense(1000, input_dim=70534, init= 'normal', activation= 'relu'))\n",
    "    #model.add(Dense(300, input_shape=(70534,), init= 'normal' , activation= 'relu' ))\n",
    "    #model.add(Dense(300, input_dim=70534, init= 'normal' , activation= 'relu' ))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, init= 'normal', activation= 'relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, init= 'normal', activation= 'relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init= 'normal' , activation= 'sigmoid'))    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='categorical', metrics=['accuracy'])\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "# Fit the model\n",
    "hist = model.fit(X, encoded_Y, validation_split=0.30, nb_epoch=50, batch_size=5, \n",
    "          callbacks=callbacks_list, verbose=2) \n",
    "\n",
    "#score = model.evaluate(X_test4.as_matrix(), Y_test, show_accuracy=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd8f6b0c290>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEeCAYAAACKQGL2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXf3RklBptAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8f5235610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Training performance',fontsize=14)\n",
    "plt.plot(hist.epoch, hist.history['loss'],'--', label='train_loss', lw=2)\n",
    "plt.plot(hist.epoch, hist.history['val_loss'], label='val_loss', lw=2)\n",
    "plt.legend()\n",
    "plt.xlabel('epoch', fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 70534)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "X_ = X.reshape(68,70534,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((47, 70534), (21, 70534))\n"
     ]
    }
   ],
   "source": [
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = cross_validation.train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "print(X_train_1.shape, X_test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = X_train_1.reshape(47,70534,1)\n",
    "X_test_2 = X_test_1.reshape(21,70534,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 47 samples, validate on 21 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 0.6498 - acc: 0.7447 - val_loss: 0.7561 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "0s - loss: 0.6321 - acc: 0.7234 - val_loss: 0.8740 - val_acc: 0.3333\n",
      "Epoch 3/10\n",
      "0s - loss: 0.5756 - acc: 0.7234 - val_loss: 3.2483 - val_acc: 0.3333\n",
      "Epoch 4/10\n",
      "0s - loss: 0.5220 - acc: 0.7660 - val_loss: 0.6508 - val_acc: 0.7619\n",
      "Epoch 5/10\n",
      "0s - loss: 0.5329 - acc: 0.7234 - val_loss: 2.1304 - val_acc: 0.3333\n",
      "Epoch 6/10\n",
      "0s - loss: 0.5676 - acc: 0.7234 - val_loss: 8.5216 - val_acc: 0.3333\n",
      "Epoch 7/10\n",
      "0s - loss: 0.5456 - acc: 0.8085 - val_loss: 0.6394 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "0s - loss: 0.6488 - acc: 0.7234 - val_loss: 0.5930 - val_acc: 0.7143\n",
      "Epoch 9/10\n",
      "0s - loss: 0.4348 - acc: 0.8723 - val_loss: 0.7836 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "0s - loss: 0.4295 - acc: 0.8723 - val_loss: 0.6639 - val_acc: 0.8571\n",
      "20/21 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "#1D CNN\n",
    "\n",
    "# 1D conv\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 5\n",
    "input_length = X.shape[1]\n",
    "nb_epoch = 10\n",
    "nb_classes = 2\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "#encoded_Y\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "Y_train_2 = np_utils.to_categorical(Y_train_1)\n",
    "Y_test_2  = np_utils.to_categorical(Y_test_1)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution1D(nb_filter=64,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        input_dim=1, \n",
    "                        input_length=input_length)) \n",
    "                       \n",
    "model.add(Convolution1D(nb_filter=32,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu'))\n",
    "                        \n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "model.add(Convolution1D(nb_filter=16,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu'))\n",
    "\n",
    "model.add(Convolution1D(nb_filter=16,\n",
    "                        filter_length=3,\n",
    "                        border_mode='same',\n",
    "                        activation='relu'))\n",
    "\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# Adding GRU layer \n",
    "#model.add(GRU(output_dim=14))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "#print_layers_shapes(model)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(nb_classes)) \n",
    "#model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "#print_layers_shapes(model)\n",
    "\n",
    "rms = RMSprop()\n",
    "model.compile(loss='binary_crossentropy', optimizer=rms, class_mode='categorical', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "history = model.fit(X_train_2, Y_train_2, batch_size=batch_size, \n",
    "                    nb_epoch=nb_epoch, show_accuracy=True, validation_data=(X_test_2, Y_test_2),verbose=2)\n",
    "\n",
    "score, acc = model.evaluate(X_test_2, Y_test_2, batch_size=batch_size, show_accuracy=True)\n",
    "\n",
    "#print('Test accuracy:', acc)\n",
    "#print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test_2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
